# ğŸ–¼ï¸ Sketch-To-Pic: A Learning-Through-Building ML Journey  
### ğŸŒ± Seasons of Code 2025 Â· ML Project Repository

Hey there! ğŸ‘‹  
This repo is my digital notebook for the **Seasons of Code 2025**, where Iâ€™m exploring and applying machine learning and deep learning techniques through focused study and mini-projects. Rather than just watching tutorials, Iâ€™m learning by building â€” one concept, one model, and one experiment at a time.

---

## ğŸ” What This Project Is About

Sketch-To-Pic is both a goal and a theme. The final aim is to build a system that turns sketches into realistic images using generative models. But before getting there, I wanted to make sure I had my fundamentals covered â€” from classical ML to modern deep nets.

So this repo contains:
- Notes and summaries of what I studied  
- Code notebooks for algorithms and models I implemented  
- Mini-projects that helped reinforce those ideas  

---

## ğŸ§  Core Concepts Covered

### 1. **Math Behind ML**
Diving into ML without understanding the math felt like walking blindfolded. So I took the time to study:
- Vectors, matrices, and their operations  
- Dot products, norms, and projections  
- Eigenstuff (eigenvalues/eigenvectors)  
- Decompositions like PCA and SVD  

These concepts really came alive when I used them to reduce dimensions or interpret model behavior.

---

### 2. **Classical ML Algorithms**
Before touching neural networks, I worked on traditional models to understand decision boundaries and performance trade-offs:
- Logistic Regression for binary classification  
- Naive Bayes for quick baseline modeling  
- k-Nearest Neighbors for simplicity and intuition  
- LDA and QDA for probabilistic approaches to class separation  
- Support Vector Machines with different kernels  

---

### 3. **Tree-Based Thinking**
One of my favorite sections â€” intuitive and powerful:
- Built decision trees from scratch  
- Learned how pruning reduces overfitting  
- Used Random Forests to boost performance  
- Played with feature importance visualizations  

---

## âš™ï¸ Stepping Into Deep Learning

With the classical models explored, I moved on to the world of neural networks. My key takeaways:
- Built feedforward networks with backpropagation  
- Understood how optimizers affect training  
- Explored how different activation functions influence learning  
- Tuned models with learning rates, batch sizes, and epochs  

---

### ğŸ”¬ Architectures Iâ€™ve Implemented

- **MLPs** for structured input  
- **CNNs** for spatial patterns in image data  
- **RNNs with GRUs** for handling sequences  
- **GANs** (the exciting part!) â€” where one network learns to fool another  

---

## ğŸ“Š Measuring What Matters

Learning to build models is one thing â€” understanding their performance is another. So I focused on:
- Evaluating predictions using confusion matrices and accuracy scores  
- Monitoring training vs validation losses  
- Plotting ROC curves and computing AUC  
- Using PCA and t-SNE to explore high-dimensional data in 2D  

---

## ğŸ› ï¸ Tools and Libraries I Worked With

- **Python** as my main language  
- **NumPy** and **pandas** for data manipulation  
- **scikit-learn** for classical ML models  
- **TensorFlow** and **Keras** for deep learning  
- **matplotlib** and **seaborn** for visualizations  
- **Jupyter Notebooks** for interactive experimentation  
- **GitHub** to document everything and track progress  

---

## ğŸ§ª Mini Projects

### ğŸ§® MNIST Digit Classifier  
- Compared MLPs, CNNs, and RNNs on handwritten digits  
- Tuned model architectures and visualized learning performance  
- Evaluated models using AUC and ROC plots  

### ğŸ· Wine Quality Predictor  
- Used classical ML models (SVM, QDA, KNN, etc.)  
- Applied PCA to visualize class clusters  
- Measured performance with accuracy and confusion matrices  


